{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lckLk1Ryaq5h",
        "colab_type": "code",
        "outputId": "b4ea419d-e9f2-46fe-df9f-e8fac917bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import tensorflow and tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# import random stuff that hopefully?? is going to be useful\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cqj6azWaq5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.0002\n",
        "n_z_input = 100\n",
        "\n",
        "# number of epochs and iterations per epoch\n",
        "train_epoch = 40\n",
        "iterations_per_epoch = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpSGD67yaq5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# takes in a series of graph_images ([#images, :, :, 0]) and plot them\n",
        "# each image is displayed as a 50 by 50 pixel image\n",
        "# where images are laid out in rows of 10 images and columns of 20\n",
        "def display_graph(graph_images, title, shape=(10, 20), image_size=(50, 50)):\n",
        "    fig = plt.figure(figsize=image_size) # define figure\n",
        "    plt.title(title) # define title\n",
        "    plt.axis('off') # remove axis\n",
        "    for i in range(0, shape[0] * shape[1]):\n",
        "        img = graph_images[i, :, :, :].astype(np.int32) # added this\n",
        "        fig.add_subplot(shape[0], shape[1], i + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ox_KVChaq5z",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFnGhn9B40h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_block(input, features):\n",
        "    out1 = tf.layers.conv2d(input, features, (3, 3), (1, 1), 'same')\n",
        "    out2 = tf.layers.conv2d(out1, features, (3, 3), (1, 1), 'same')\n",
        "    return out2 + input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqCdsVP3aq50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator and discriminator \n",
        "def generator(features, name, reuse=False):\n",
        "    with tf.variable_scope('generator' + name, reuse=reuse):\n",
        "        # tf.layers.conv2d_transpose\n",
        "        l1 = tf.layers.conv2d_transpose(features, 512, [4, 4], strides=(1, 1), activation=tf.nn.relu, padding=\"VALID\")\n",
        "        l2 = tf.layers.conv2d_transpose(l1, 256, [4, 4], strides=(4, 4), activation=tf.nn.relu, padding=\"SAME\")\n",
        "        l3 = tf.layers.conv2d_transpose(l2, 128, [4, 4], strides=(2, 2), activation=tf.nn.relu, padding=\"SAME\")\n",
        "        conv4 = tf.layers.conv2d_transpose(l3, 1, [4, 4], strides=(2, 2), padding=\"SAME\")\n",
        "        # # kernels | kernel dimension | stride | padding | activation\n",
        "        #  512        [4, 4]            (1, 1),   \"VALID\"   relu\n",
        "        #  256        [4, 4]            (4, 4),   \"SAME\"    relu\n",
        "        #  128        [4, 4]            (2, 2),   \"SAME\"    relu\n",
        "        #  1          [4, 4]            (2, 2),   \"SAME\"    relu\n",
        "        # conv3 is the output of the last conv2d_transpose layer\n",
        "#         l1 = tf.layers.conv2d_transpose(features, 1024, 4, strides=1, activation=\"relu\", padding=\"VALID\")\n",
        "#         # 512, (4, 4), stride = (2, 2), relu, padding=\"SAME\"\n",
        "#         l2 = tf.layers.conv2d_transpose(l1, 512, 4, strides=2, activation=\"relu\", padding=\"SAME\")\n",
        "#         # 256, (4, 4), stride = (2, 2), relu, padding=\"SAME\"\n",
        "#         l3 = tf.layers.conv2d_transpose(l2, 256, 4, strides=2, activation=\"relu\", padding=\"SAME\")\n",
        "#         # 128, (4, 4), stride = (2, 2), relu, padding=\"SAME\"\n",
        "#         l4 = tf.layers.conv2d_transpose(l3, 128, 4, strides=2, activation=\"relu\", padding=\"SAME\")\n",
        "#         # 1 kernel, (4, 4), stride = 2, tanh, padding=\"SAME\"\n",
        "#         final = tf.layers.conv2d_transpose(l4, 512, 4, strides=2, activation=\"tanh\", padding=\"SAME\")\n",
        "        return tf.nn.tanh(conv4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD_5-nuIaq54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(features, name, reuse=False):\n",
        "    with tf.variable_scope('discriminator' + name, reuse=reuse):\n",
        "        # # kernels | kernel dimension | stride | padding | activation\n",
        "        #  128        [4, 4]             (2, 2),  \"SAME\"    leaky_relu\n",
        "        #  256        [4, 4]             (2, 2),  \"SAME\"    leaky_relu\n",
        "        #  512        [4, 4]             (4, 4),  \"SAME\"    leaky_relu\n",
        "        #  1024       [3, 3]             (1, 1),  \"VALID\"   leaky_relu\n",
        "        l1 = tf.layers.conv2d(features, 128, [4, 4], strides=(2, 2), activation=tf.nn.leaky_relu, padding=\"SAME\")\n",
        "        l2 = tf.layers.conv2d(l1, 256, [4, 4], strides=(2, 2), activation=tf.nn.leaky_relu, padding=\"SAME\")\n",
        "        l3 = tf.layers.conv2d(l2, 512, [4, 4], strides=(4, 4), activation=tf.nn.leaky_relu, padding=\"SAME\")\n",
        "        conv4 = tf.layers.conv2d(l3, 1024, [3, 3], strides=(1, 1), activation=tf.nn.leaky_relu, padding=\"VALID\")\n",
        "        # conv4 is the output of the last conv2d_transpose layer\n",
        "#         # 128 kernels, (4, 4) kernel size, stride 2, relu, padding = \"SAME\"\n",
        "#         layer_1 = tf.layers.conv2d(features, 128, 4, strides=2, activation=\"relu\", padding=\"SAME\") \n",
        "#         # 256 kernels, (4, 4) kernel size, stride 2, relu, padding = \"SAME\"\n",
        "#         layer_2 = tf.layers.conv2d(layer_1, 256, 4, strides=2, activation=\"relu\", padding=\"SAME\") \n",
        "#         # 512 kernels, (4, 4) kernel size, stride 2, relu, padding = \"SAME\"\n",
        "#         layer_3 = tf.layers.conv2d(layer_2, 512, 4, strides=2, activation=\"relu\", padding=\"SAME\") \n",
        "#         # 1024 kernels, (4, 4) kernel size, stride 2, relu, padding = \"SAME\"\n",
        "#         layer_4 = tf.layers.conv2d(layer_3, 1024, 4, strides=2, activation=\"relu\", padding=\"SAME\") \n",
        "#         # 1 kernel, (4, 4) kernel size, stride 1, relu, padding = \"VALID\"\n",
        "#         final = tf.layers.conv2d(layer_4, 1, 4, strides=1, activation=\"relu\", padding=\"VALID\")\n",
        "        flatten = tf.contrib.layers.flatten(conv4)\n",
        "        logits = tf.layers.dense(flatten, 1)\n",
        "        # use sigmoid to squash output into a probability\n",
        "        output = tf.nn.sigmoid(logits) \n",
        "        \n",
        "        return output, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjymVoymaq59",
        "colab_type": "text"
      },
      "source": [
        "## Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoR62-7Paq59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1 = tf.placeholder(tf.float32, shape=(None, 256, 256, 3))\n",
        "x2 = tf.placeholder(tf.float32, shape=(None, 256, 256, 3))\n",
        "# z = tf.placeholder(tf.float32, shape=(None, 256, 256, 3)) # n_z_input))\n",
        "# 3 was originally n_z_input and the 64s were 1s for z vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfO3XDa8aq6A",
        "colab_type": "code",
        "outputId": "1df43058-c86b-47d7-f546-e4eafe9c672e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "# generator is generating an image (g is the fake image)\n",
        "g1 = generator(x1, 'apple', False)\n",
        "g2 = generator(x2, 'orange', False)\n",
        "# discriminator is classifying real images \n",
        "# the first output is the probability and the second is the logits\n",
        "# to feed into sigmoid_cross_entropy_with_logits\n",
        "disc_real_1, disc_real_logits_1 = discriminator(x1, 'apple', False)\n",
        "disc_fake_1, disc_fake_logits_1 = discriminator(g1, 'apple', True)\n",
        "\n",
        "disc_real_2, disc_real_logits_2 = discriminator(x2, 'orange', False)\n",
        "disc_fake_2, disc_fake_logits_2 = discriminator(g2, 'orange', True)\n",
        "\n",
        "# get accuracy of the discriminator in\n",
        "# predicting that the image is real or fake\n",
        "\n",
        "# the goal is to get a list of 1 and 0 whether it predicted right or not\n",
        "# and use tf.reduce_mean to get mean of 1 and 0 which returns a probability\n",
        "# you can use > or < 0.5 to get true and false \n",
        "# whether it predicted right or not and use tf.cast to cast that boolean into tf.float32\n",
        "real_accuracy_1 = tf.reduce_mean(tf.cast(disc_real_1 > 0.5, tf.float32)) # TODO\n",
        "fake_accuracy_1 = tf.reduce_mean(tf.cast(disc_fake_1 < 0.5, tf.float32)) # TODO\n",
        "\n",
        "real_accuracy_2 = tf.reduce_mean(tf.cast(disc_real_2 > 0.5, tf.float32))\n",
        "fake_accuracy_2 = tf.reduce_mean(tf.cast(disc_fake_2 < 0.5, tf.float32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-43864b1d98ad>:4: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:1279: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-c789f7d11eb6>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-c789f7d11eb6>:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50201eb6df88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# to feed into sigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisc_real_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_real_logits_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apple'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdisc_fake_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_fake_logits_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apple'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdisc_real_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_real_logits_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c789f7d11eb6>\u001b[0m in \u001b[0;36mdiscriminator\u001b[0;34m(features, name, reuse)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#  512        [4, 4]             (4, 4),  \"SAME\"    leaky_relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#  1024       [3, 3]             (1, 1),  \"VALID\"   leaky_relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m       _scope=name)\n\u001b[0;32m--> 424\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \"\"\"\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    871\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    872\u001b[0m                          \u001b[0;34m\" and found shape %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                          (name, shape, found_var.get_shape()))\n\u001b[0m\u001b[1;32m    874\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Trying to share variable discriminatorapple/conv2d/kernel, but specified shape (4, 4, 1, 128) and found shape (4, 4, 3, 128)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVq1JQrEaq6D",
        "colab_type": "text"
      },
      "source": [
        "Important! The labels are tf.ones([batch_size, 1]) instead of tf.ones([batch_size, 1, 1, 1]) because the output of the discriminator is a single probability i.e. [batch_size, 0.5] instead of [batch_size, 0.5, 1, 1] because its no longer the output of a conv layer but the output of a dense layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd-d1TOHaq6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function for discriminator_1\n",
        "disc_loss_real_1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits_1, labels=tf.ones([batch_size, 1]))\n",
        "disc_loss_real_1 = tf.reduce_mean(disc_loss_real_1)\n",
        "\n",
        "disc_loss_fake_1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits_1, labels=tf.zeros([batch_size, 1]))\n",
        "disc_loss_fake_1 = tf.reduce_mean(disc_loss_fake_1)\n",
        "\n",
        "disc_loss_total_1 = disc_loss_real_1 + disc_loss_fake_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvIepzZv4T9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function for discriminator_2\n",
        "disc_loss_real_2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits_2, labels=tf.ones([batch_size, 1]))\n",
        "disc_loss_real_2 = tf.reduce_mean(disc_loss_real_2)\n",
        "\n",
        "disc_loss_fake_2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits_2, labels=tf.zeros([batch_size, 1]))\n",
        "disc_loss_fake_2 = tf.reduce_mean(disc_loss_fake_2)\n",
        " \n",
        "disc_loss_total_2 = disc_loss_real_2 + disc_loss_fake_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGlYQmguaq6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function for generator\n",
        "gen_loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits_1, labels=tf.ones([batch_size, 1]))\n",
        "gen_loss_1 = tf.reduce_mean(gen_loss_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir5SA7pH5fYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function for generator_2\n",
        "gen_loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits_2, labels=tf.ones([batch_size, 1]))\n",
        "gen_loss_2 = tf.reduce_mean(gen_loss_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8y3eFOpaq6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_vars = tf.trainable_variables()\n",
        "disc_var_1 = [var for var in t_vars if var.name.startswith('discriminatorapple')]\n",
        "gen_var_1 = [var for var in t_vars if var.name.startswith('generatorapple')]\n",
        "\n",
        "disc_var_2 = [var for var in t_vars if var.name.startswith('discriminatororange')] # HAVE TO CHANGE SOMETHING STILL\n",
        "\n",
        "gen_var_2 = [var for var in t_vars if var.name.startswith('generatororange')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QPFOOAWaq6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "    gen_optimizer_1 = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(gen_loss_1, var_list=gen_var_1)\n",
        "    disc_optimizer_1 = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(disc_loss_total_1, var_list=disc_var_1)\n",
        "    gen_optimizer_2 = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(gen_loss_2, var_list=gen_var_2)\n",
        "    disc_optimizer_2 = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(disc_loss_total_2, var_list=disc_var_2)\n",
        "    real_fake_loss = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(## (real_apple - fake_apple)^2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVhoLTff5_HP",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcgdpxCmaq6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset, metadata = tfds.load('cycle_gan/apple2orange', with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7bM3nBRaq6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apple_train_dataset, orange_train_dataset, apple_test_dataset, orange_test_dataset = datasets[\"TRAINA\"], datasets[\"TRAINB\"], datasets[\"TESTA\"], datasets[\"TESTB\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozfg8u4zaq6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_map(inputs):\n",
        "    img = inputs['image']\n",
        "    # 1. cast the img into float32\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = tf.subtract(img, 0.5)\n",
        "    img = tf.divide(img, 0.5)\n",
        "    # 2. image values are from 0 to 1. convert to range -1 to 1\n",
        "    # by subtracting by a decimal so its range -0.5 to 0.5 and\n",
        "    # then divide by a decimal so its not -1 to 1\n",
        "    # img = tf.math.tanh(img)\n",
        "    # 3. use tf.image.resize to conver the image into a 64 by 64 image\n",
        "    img = tf.image.resize(img, [256, 256])\n",
        "    return img #tanh???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0TiReVmaq6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apple_train_dataset = apple_train_dataset.map(apply_map)\n",
        "apple_train_dataset = apple_train_dataset.shuffle(1024)\n",
        "apple_train_dataset = apple_train_dataset.batch(batch_size)\n",
        "\n",
        "apple_iterator = apple_train_dataset.make_initializable_iterator()\n",
        "apple_batch = apple_iterator.get_next()\n",
        "\n",
        "orange_train_dataset = orange_train_dataset.map(apply_map)\n",
        "orange_train_dataset = orange_train_dataset.shuffle(1024)\n",
        "orange_train_dataset = orange_train_dataset.batch(batch_size)\n",
        "\n",
        "orange_iterator = orange_train_dataset.make_initializable_iterator()\n",
        "orange_batch = orange_iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X26JF8eKaq6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# z_test input into the generator with batch_size 10 \n",
        "z_test = np.random.normal(0, 1, (10, 1, 1, n_z_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5qvWQTNaq6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "with tf.Session(config=config) as sess:\n",
        "    # initialize variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(train_epoch):\n",
        "        print(\"Starting epoch: \" + str(epoch))\n",
        "        # initialize iterator\n",
        "        sess.run(iterator.initializer)\n",
        "        step = 0\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                tra_images = sess.run(batch) # grab the batch\n",
        "\n",
        "                # makes sure the batch_size is 64\n",
        "                if tra_images.shape[0] != 64:\n",
        "                    break\n",
        "\n",
        "                # do the generator separately?\n",
        "                \n",
        "                # then do the discriminator?\n",
        "                    \n",
        "                # z vector of size batch_size\n",
        "                z_batch = np.random.normal(0, 1, (batch_size, 1, 1, n_z_input))\n",
        "                \n",
        "                acc_fake, acc_real, loss_d, _, loss_g, _ = sess.run(\n",
        "                  [fake_accuracy, real_accuracy, disc_loss_total, disc_optimizer, gen_loss, gen_optimizer],\n",
        "                  feed_dict={x: tra_images, z: z_batch})\n",
        "                step += 1\n",
        "\n",
        "                if step % 200 == 0:\n",
        "                    generated_images = sess.run(g, feed_dict={z :z_test}) # CHECK THIS\n",
        "                    display_graph(generated_images, \"MNIST Images\", (5, 2), image_size=(20, 20))\n",
        "                print('Epoch: %d, Iteration: %d, loss_d: %.3f, loss_g: %.3f, acc_fake: %.3f, acc_real: %.3f' % (\n",
        "                        epoch, step, loss_d, loss_g, acc_fake, acc_real))\n",
        "                \n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g4ZBfYzaq6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}